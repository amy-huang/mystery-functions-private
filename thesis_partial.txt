%++++++++++++++++++++++++++++++++++++++++
% Don't modify this section unless you know what you're doing!
\documentclass[letterpaper,12pt]{article}
\usepackage{fullpage}
\renewcommand{\baselinestretch}{1.5} 
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{diagbox}
\usepackage[table]{xcolor}
\usepackage[margin=1in,letterpaper]{geometry} % decreases margins
\usepackage{textcomp}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\makeatletter
\let\NAT@parse\undefined
\makeatother
\usepackage{hyperref}
\hypersetup{
	pdftitle={PhD Thesis Proposal Hazoor Ahmad PhDEE17004},
   colorlinks=true,
   citecolor=blue,
   linkcolor=blue,
   urlcolor=blue
}

\usepackage{float}
%++++++++++++++++++++++++++++++++++++++++
\begin{document}
\begin{titlepage}

\begin{center}
\LARGE \textbf {Mystery Functions}\\
[\baselineskip]
\large \textbf{by Amy Huang}

% \large \textbf {Subtitle if any}\\
\normalsize Computer Science Honors Thesis\\
\normalsize Brown University, May 2020\\
[\baselineskip]

\includegraphics[scale=0.5]{images/brownlogo.png}\\[0.1in]


%  \small
%       \textit{\textbf{Submitted in partial fulfillment of
%         the requirements for the award of the degree of}}\\[0.3in]
        
% \normalsize
%       \textbf{Master/DOCTOR OF PHILOSOPHY \\IN\\ ELECTRICAL ENGINEERING}\\[0.5in]

% Submitted by

\vspace{.3in}
\large Advisor \\
{\normalsize\textbf{Tim Nelson}}\\[0.1in]
\small Brown University Computer Science\\
[\baselineskip]

\large Reader \\ 
{\normalsize\textbf{Shriram Krishnamurthi}}\\[0.1in]
\small Brown University Computer Science\\
[\baselineskip]

\large In collaboration with \\
{\normalsize\textbf{Rob Goldstone}}\\[0.1in]
\small Indiana University Psychological and Brain Sciences

\vfill

\end{center}

\end{titlepage}
\begin{abstract}
TODO: Abstract; write after conclusions.
\newline
\newline
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
% \keywords{datasets, neural networks, gaze detection, text tagging}


%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
People form theories based on observations they make about their surroundings, and often seek to systematically collect data for the purpose of refining those theories. How people go about designing and executing this data collection is of great interest in not only psychology and cognitive science, but also education. (REF: Rob-related work.)

One way to study the process of theory formation is to create an all-knowing source of truth, or "oracle", that subjects are directed to query and find out the cause of some phenomenon they observe in their environment. The oracle knows what the cause is and will answer any question about it. Further, the subject can at any time guess the cause, and the oracle will respond whether it is correct. One can thus observe how people design experiments and react to the results - the queries they make of the oracle, before and after receiving feedback on their guesses.

In our project, the natural phenomenon to be understood is a computational function that takes in data, performs computation, and returns the result. A web application serves as the oracle. The queries subjects can make are of the form "What does this function output for an input of X?", where X is some valid input value. The subject's goal is to guess what the function is.

We call this approach Mystery Functions, and it has several desirable qualities as a study framework. It defines a single unambiguous, objective truth to be discovered by scientific inquiry that we have complete control and knowledge of. Subjects receive instant, unambiguous, and correct responses to queries on what outputs result from the inputs they give, and also the implicit query they make about what the function does - when they answer a quiz question. They leave behind an unambiguous and unbiased record of the inquiry they performed. Accessing the study web application is convenient and scales well to large subject populations.

With this project we contribute the datasets of the actions subjects undertook during their sessions and their labeled written guesses about the functions, and concrete insight into patterns in their methods of inquiry. (TODO: after writing results section, be more specific and summarize what this insight is)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% what are we modeling
% \section{Ideal Study Conditions}
% powerfulness of web app, diversity of people, scale/incentives. compromises we made instead that best emulate the perfect study to investigate our questions.

\section{Study Design}
\begin{figure}[H]
  \centering
  \includegraphics[width=.9\textwidth]{statemachine.jpeg}
  \caption{Workflow of a mystery functions session.}
%   \label{fig:sub1}
\end{figure}

Our study is designed as follows. Subjects are given the signature of a computational function: how many inputs it takes and outputs it produces, and their types. They are then free to evaluate inputs, given that they are of valid type according to the function signature. There is no limit to how many inputs one can evaluate.

At any point, they may choose to submit a guess about the function and verify whether it is correct by taking a quiz. They are presented 3 hard-coded inputs to the mystery function, one at a time, and asked what the corresponding outputs are. The input values for these quiz questions were chosen to cover the range of possible inputs such that it is unlikely that the subject can guess the answers correctly without truly understanding the nature of the function.

Quiz questions are presented one at a time in order every time, and if one is answered incorrectly, the subject is not allowed to go on. They must choose to go back to evaluating inputs, or give up and move on to the next function. If all quiz questions are completed successfully, the subject moves on to the next function as well.

We presented university students with five mystery functions, which we estimated to take around 50 minutes on average to complete.

\subsection{The Web Application}

The structure of the web application is as follows. A subject first opens the URL to the web application. The initial page describes the mystery functions activity and an estimation of how much time it will take (REF: 1st page). Once the subject submits an ID that will uniquely identify the data collected from their session, they are shown the evaluation screen for the first function (REF: evaluation screen). 

In the top left corner of the screen is a description of the function signature, and how to specify input values in text according to the value type (REF: add description of all the types in the appendix). In the bottom left corner, there is a tabbed window. The first tab is for evaluating inputs: there is a text box for each input, and pressing the ENTER key will submit. On the right side of the screen, the submitted input and corresponding output will be displayed.

The second tab of the bottom left window is for submitting guesses about the function. Subjects articulate in text what they think the function is, and once they submit their guess, are taken to the quiz screens (REF: quiz screen).

For each quiz question, they are prompted to submit the appropriate output value for each input. If they answer incorrectly, they have the option to either return to the evaluation screen, or give up guessing and move to a screen describing what the function does (REF: quiz wrong answer screen, function answer screen). If they answer correctly, they go to the next quiz question; if all questions are completed successfully, they move to the function description screen immediately. Once the function description screen is reached, subjects can move on to the next function and are not allowed to go back to previous functions. After all functions are completed, a screen indicating successful completion is shown (REF: completion screen).

\begin{quote}
We now describe an example workflow of a mystery functions session, with a mystery function far simpler than the functions used in the study.

An example subject evaluated a sequence of inputs, attempted the quiz and got 2 out of 3 questions correct, evaluated a few more inputs, and finally passed the quiz successfully. 

Let the mystery function be |x| + 1: given an integer x, the function returns the absolute value of x plus one, and the quiz inputs are 33, 10, and -3. The first sequence of inputs evaluated is 1, 2, 3, 4, 5, 6, 0. In the first quiz attempt, the subject gave the correct output for inputs 33 and 10, but not -3, because they thought the function was x + 1.

Afterwards, the subject evaluates -3 and -2, and answer all the quiz questions correctly in their next attempt. Their written answer reflects true understanding of the function: "absolute value of the input plus 1". 
\end{quote}


\subsection{Data Collection and Labeling}
% Include figure of schema. Explains what information we collected and how it was framed for analysis.
As the user is completing the study, their actions are logged to a database. The data we record are: inputs evaluated, quiz question responses, and the final written guess about the function submitted prior to the last quiz attempt made before moving on to the next function. Each action is recorded as a database row, identified uniquely by the user ID, function name, and local timestamp. No actions are logged for a function after a subject sees the answer for a function, so we can safely assume that the actions that are recorded are done without knowing for sure what the function is.
% timestamps?

Additionally, we classified the final guesses from each session qualitatively. Because evaluating written answers cannot easily be automated, and is highly subjective, we constructed a system for labeling answers based on aspects that differentiated them from others - and also based on how correct the answer given was.

We gave 4 different rankings of correctness from least to most correct, function-independent labels, and function-dependent labels. To ensure that label usage was unambiguous, we created decision trees to describe the conditions for each of them. One begins at the root question, and answers all child questions until the leaf categories are reached, indicating that the corresponding label should be given. The diamond-tipped lines lead to mutually exclusive answers to the parent question, and the arrows point to questions that must be answered if the parent condition applies. 

TODO: data sanitizing choices; ignoring people who didn't evaluate inputs. Some subjects did not complete all of the mystery functions presented to them, so the total responses for each function and matched pairs of functions may not add up to the total number of subjects. Also, not all respondents completed 4 mystery functions.

\subsection{Subjects}
Our participants came from two distinct sources. At Indiana University, students can receive course credit by participating in studies administered by the Department of Psychological and Brain Sciences (PBS). Students enrolled in approved PBS courses are eligible to sign up for the program, and must complete a number of 50 minute studies over the course of a semester. The functions used for the mystery functions study were chosen to take approximately 10 minutes each so that the total time taken would be approximately 50 minutes to accommodate this constraint. (CITATION for this info)

At Brown University, Introduction to Software Engineering (CSCI 0320) students were given the option of completing our study for extra credit. The course involves substantial programming projects - completed individually and also in pairs and larger teams. As of the spring semester of the 2019-2020 school year, most students in the course are using it as a requirement for the computer science major. Additionally, most of the students are 2nd or 3rd year undergraduates.

We administered the Indiana University portion of the study in the fall semester of the 2019-2020 school year, and the Brown portion in the spring semester. From Indiana University, there were a total of 62 respondents, and from Brown University there were 80 who submitted a guess about at least one of the functions. Table~\ref{fig:numsubjects} below shows the number of completed sessions for each function, in which the subject submitted a written guess about the function.

\begin{center}
\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|}
\hline
                 & Indiana & Brown \\ \hline
Average          & 40         & 32       \\ \hline
Median           & 39         & 46       \\ \hline
SumParityBool    & 33         & 38       \\ \hline
SumParityInt     & 44         & 39       \\ \hline
SumBetween       & 77         & 70       \\ \hline
MultiplyAndHalve & 82         & 74       \\ \hline
Total            & 68         & 80       \\ \hline
\end{tabular}
\end{table}
\caption{Number of sessions recorded per function per subject pool.}
\label{fig:numsubjects}    
\end{center}

\subsection{Design Limitations}
TODO: Explain fully the limitations of our study design and implementation like the below points in full paragraphs and not a bullet list.
\begin{itemize}
    \item Written guesses don't perfectly convey the subject's understanding of the function like a program would. However, we wanted to have subjects without programming experience, and did not find a satisfactory alternate solution.
    \item The quiz doesn't perfectly confirm or deny the subject's guess about the function, as questions can be guessed correctly. For output types with few possible values like booleans, this makes it less clear to both subject and us whether or not they were right. This may also lead to subjects trying to game the quiz and brute force right answers.
    \item The web application uses cookies to prevent subjects from doing the study multiple times, and from data for a function session from being recorded after they have seen the description of it. However, reloading and returning to previous pages is still possible, as is clearing browser cookies. (Rephrase: the hope is that students aren't incentivized so strongly to game the activity that the vast majority will not bother)
    \item Labeling written guesses about the functions is subjective, error prone and tedious. We minimized the potential differences in correctness rating assignments by constructing well-defined rules for assigning labels, which are listed in the appendix (REF).
    \item In the fall iteration of the study at Indiana University, subjects were allowed to evaluate inputs they had seen during a quiz attempt before; in the spring iteration at Brown, they were not. 
\end{itemize}

\subsection{Functions Given}
The mystery functions included were chosen to represent a wide range of possible functions, and take on average 10 minutes each to complete for students to complete, whether they knew how to program or not. They are presented in random order.

A preliminary trial study run was conducted on Amazon Mechanical Turk to gauge the difficulty of two versions of a potential candidate function \textbf{IsPalindrome}. The first version of IsPalindrome takes in a list of integers, and outputs a boolean: whether the input list is palindromic or not. The second version takes in a string, and returns whether the string is palindromic. Each version of IsPalindrome was assigned to 5 Turk workers with programming experience. On both versions of the function, workers took 5 minutes or less to complete the session, but fewer than half gave correct final guesses about the function. As a result, we chose simpler functions for the study.

Below is a description of the functions used in the study. (REF: to full description with quiz inputs and outputs in appendix)
\begin{itemize}
    \item\textbf{Average} takes in a list of integers, and returns an integer that is the average of the input list.
    \item\textbf{Median} takes in a list of integers, and returns an integer that is the median of numbers in list. This is the middle number of sorted input list its length is odd; average of the two middle numbers otherwise.
    \item\textbf{SumParityBool} takes in a list of integers, and returns the boolean value true if the parity of list is 1; false otherwise.
    \item\textbf{SumParityInt} takes in a list of integers, and returns the parity of the sum of numbers in list: the remainder of sum divided by 2.
    \item\textbf{SumBetween} takes in two integers, and returns the sum of integers between first and second input integers, inclusive. If the inputs are equal, the result is that value; if the first is larger, the result is 0.
    \item\textbf{MultiplyAndHalve} takes in an integer that we will call $x$. It outputs $\frac{x \cdot (x - 1) }{2}$.
\end{itemize}

Some of the functions come in matched pairs such that every subject was only shown one of them. Upon visiting the web application URL, one of the matched pair functions is chosen uniformly at random for that subject. We wanted to see the difference in behavior of subjects guessing slightly different functions, to see if there was a significant effect of those differences. The first matched pair is \textbf{Average} and \textbf{Median}, which perform similar but distinct calculations; the second is \textbf{SumParityBool} and \textbf{SumParityInt}, which differ in output type.

\section{Mystery Predicates}

In addition to the Brown and Indiana University studies, we also designed and administered a study specifically for Brown students who took Logic for Systems (CSCI 1950Y), an upper level computer science class that serves as an introduction to formal methods. Most students are 2nd or 3rd year undergraduates.

Over the course of 1950Y, students learn to use model finders, which are tools to find concrete structures made up of sets and relations that satisfy sets of constraints expressed as predicates. They are used extensively in software and hardware verification, as well as program synthesis. Students practice writing not only constraint predicates, but also concrete instances of structures.

As a result, it is feasible to present to them mystery \textit{predicates} that take in a concrete structure, and output a single boolean value. We defined a type of concrete structure that represents a graph: a set named \textbf{Node} that contains uniquely named nodes, and a binary relation named \textbf{Edges} of ordered pairs of nodes that represent edges. To construct an input to evaluate one of our mystery predicates on, students specify what nodes exist with the Node set, and what edges exist with the Edges binary relation. (REF: full description of mystery predicates in the appendix.)

Besides the change of input type, the mystery predicates is identical to mystery functions: input evaluation, guessing the predicate, and quiz attempts are the same. We administered the mystery predicates study during the spring semester of the 2019-2020 school year, and have the resulting dataset to contribute to future work, though no analysis was done using it over the course of this project.

\section{Results}

\subsection{Matched Pairs}
...

\subsection{General Trends}

Brown students on average did better than Indiana students. Each function guess was given a correctness score between 1 to 4 inclusive, 4 being the highest score. The median score for a guess written by a Brown student is 4, while the median score for a guess written by an Indiana student is 2.5. Because the Indiana students are not expected to have programming experience, this is expected. Though performance differed significantly across subject pools, many trends in behavior were the same. 

In both groups, subjects who scored the lowest and the highest - receiving scores 1 or 4 - made the fewest quiz attempts on average across all functions; those who scored a 4 however evaluated 1.5 times more inputs than those who scored a 1. (TODO: takeaway?) Furthermore, most subjects only ever attempted the quiz once, and if they did make multiple attempts, on average most of the inputs evaluated are made before the first quiz attempt. 67 percent of Indiana students and 86 percent of Brown students only attempted the quiz once. Indiana students who made multiple attempts evaluated 62 percent of all their inputs before the first ever quiz attempt; for Brown students, 61 percent. Since most of the inputs that subjects evaluate in a session are chosen before seeing any quiz question inputs, we believe that better scores do in fact correlate with better ability to choose inputs and analyze the resulting outputs.

Across all subjects, the average correctness score that a subject gets on a function doesn't appear to correlate with whether the subject saw the function earlier or later. The average scores of those who completed two functions are 2.48 and 2.29; those who completed three functions had average scores of 3.15, 2.90, and 3.08; those who completed all four had average scores of 3.39, 3.25, 3.25, and 3.20. From our limited number of sessions per subject, we do not see evidence of better performance on later functions seen.

\subsection{Choosing Inputs}
% motivation of looking at operation chains - easily identify strategies being applied to generating inputs to evaluate
% define in terms of edit distance, list of ints/2 inputs/integer
In order to investigate how subjects chose inputs to evaluate, we did analysis on the changes that would be applied on each input evaluated to produce the next input evaluated during a subject's session. The changes are defined in terms of the minimum edit operations needed to transform the string representation of one input to the next. The base operations are the string operations counted to calculate edit distance: inserting, deleting, or replacing single characters at some valid string index.

% We defined the change from one integer to another as the edit operations needed to transform the string representation of the first integer in base 10 notation to that of the second. For example, the minimum operations needed from 6043 to 1605 are: insert the digit 1 before the 6, delete 3, and replace the 4 with 5.
For this portion of analysis, we only looked at functions that take lists of integers as inputs: Average, Median, SumParityBool, and SumParityInt. (TODO: because ints and 2 int inputs don't vary enough for this measure of difference to say much) We defined the change from one list of integers to another as the edit operations needed to transform a string representation of the first, in which one character represents each element, to the second. For example, the operations needed to transform list [112, 55, 112, 0] to [112, 33, 0] would be the same operations needed to transform string "ABAC" to "ADC". 

By defining the change between consecutive inputs this way, we can examine how subjects systematically selected inputs to evaluate. Strategies that involve repeatedly replacing a single list element to generate new inputs, or inserting an element at the end of the list, are detected. The sequence of all inputs evaluated during a session, then, can be broken up into chains of inputs in which there is only one group of minimal operations that exists between each pair of consecutive inputs. Consider the example integer sequence 1, 12, 123, 1234, 0, 1, 2, 3. The first four inputs are tied together by singular insert operations. Then, 1234 to 0 requires three deletion and one replacement. The remaining inputs are created with singular replace operations.

Below are the operation groups that make up 80 percent of operation chains across functions with list of integer inputs. Note that minimum operation groups can contain multiple of any base operation, and are unordered.
\begin{center}
\begin{table}[h]
\begin{tabular}{|l|r|r|}
\hline
Operation Group         & \multicolumn{1}{l|}{Proportion of Input Chains} & \multicolumn{1}{l|}{Average Number of Inputs} \\ \hline
replace                 & 31\%                                            & 1.43                                          \\ \hline
insert                  & 18\%                                            & 1.74                                          \\ \hline
replace replace         & 10\%                                            & 2.35                                          \\ \hline
insert replace          & 5\%                                             & 3.65                                          \\ \hline
delete                  & 4\%                                             & 1.94                                          \\ \hline
delete replace          & 3\%                                             & 2.14                                          \\ \hline
insert insert           & 3\%                                             & 1.55                                          \\ \hline
replace replace replace & 3\%                                             & 2.03                                          \\ \hline
delete delete replace   & 2\%                                             & 1.60                                          \\ \hline
\end{tabular}
\end{table}
% \caption{Breakdown of what operation groups that make up most of sequences of consecutive inputs, and the average lengths of such sequences.}
\label{fig:opgroups}   
\end{center}

Though single replace operations are the most common, the average chain length is less than two inputs long, implying that the majority of them are of length 1. However, the low average chain lengths shown above hide an important pattern in operation chain length. Across all of the functions with list of integer inputs, the lengths of adjacent operation chains tend to vary widely. The average length of all input chains is 2.3, and the difference of the length of any chain from the average chain length is 1.8. Additionally, average difference between the lengths of adjacent input chains is 2.4 - adjacent input chains tend to differ in length significantly.

The conclusion we draw from this data is that people do choose and execute different strategies of choosing inputs, often breaking a streak of using the same operations to start another one using different operations, using a single unrepeated operation group in between to choose a new input with which to start the new streak. Both Indiana and Brown students used this approach frequently.

\section{Conclusions}
TODO: conclusions; write after results are done.
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acks}
TODO: write last.
\end{acks}

\section{TODO: bibliography}
%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
% \bibliographystyle{ACM-Reference-Format}
% \bibliography{sample-base}

\section{Appendix}

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{evaluator.png}
  \caption{Evaluation screen.}
%   \label{fig:sub1}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{quiz.png}
  \caption{Quiz screen displaying options after answering incorrectly.}
%   \label{fig:sub1}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{answer.png}
  \caption{Answer screen displaying our description of the function.}
%   \label{fig:sub1}
\end{figure}

\begin{figure}[H]
\centering
\begin{subfigure}{.8\textwidth}
  \centering
  \includegraphics[width=.8\textwidth]{tutorial.jpeg}
  \caption{An example decision tree to classify the blue rectangle with a star in it. It should receive the labels WHE, WHI, WW, and SOB.}
%   \label{fig:sub1}
\end{subfigure}
\begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=\textwidth]{sumparitycodes.png}
  \caption{The decision tree for SumBetween function specific labels.}
%   \label{fig:sub2}
\end{subfigure}
% \caption{A figure with two subfigures}
% \label{fig:test}
\end{figure}

\end{document}
